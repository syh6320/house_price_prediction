---
title: "homework4_4"
author: "syh"
date: "June 1, 2017"
output: pdf_document
---

```{r}
# in this section, we are gonna try to build random forest
# also, we would like to use mlr to find optimal parameters
```

```{r}
# read all data (train + prediction) without missing value
real_all_data <- read.csv(file = "H:/kaggle/houseprice/data/real_all_data_hybrid.csv",
                          stringsAsFactors = FALSE)[,-c(1,2)]
```

```{r}
# transform sale price to log sale price
real_all_data[,"SalePrice"] <- log(real_all_data[,"SalePrice"])
```

```{r}
# we would like to train a linear regression model with regulation
# 1. convert categorical ones to factors
for(i in 1:dim(real_all_data)[2]){
  if(is.character(real_all_data[,i])){
    real_all_data[,i] <- as.factor(real_all_data[,i])
  }
}
```

```{r}
# 1. split all data into train and prediction

model_data <- real_all_data[1:1460,]

pre_x <- real_all_data[-c(1:1460),]

# 2. split model data into train and test
set.seed(1000)
train_ind <- sample(1:dim(model_data)[1], size = dim(model_data)[1] * 0.7)

train_data <- model_data[train_ind,]
test_data <- model_data[-train_ind,]
```


```{r}
library(randomForest)
library(mlr)
#create a task
traintask <- makeRegrTask(data = train_data,target = "SalePrice") 
testtask <- makeRegrTask(data = test_data,target = "SalePrice")

```

```{r}
#create learner
rf_lrn <- makeLearner("regr.randomForest",predict.type = "response")
rf_lrn$par.vals <- list(ntree = 100L, importance=TRUE)
```


```{r}
#set 5 fold cross validation
rdesc <- makeResampleDesc("CV",iters=5L)

# Fit models according to a resampling strategy.
r <- resample(learner = rf_lrn, task = traintask, resampling = rdesc, measures = mse, show.info = T)
```

```{r}
getParamSet(rf_lrn)
```


```{r}

# define parameters we want to tune -- you may want to adjust the bounds
ps <- makeParamSet(
  makeIntegerLearnerParam(id = "ntree", default = 100L, lower = 80L, upper = 150L),
  makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 50L),
  makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6, upper = 12L)
)

# random sampling of the configuration space with at most 100 samples
ctrl <- makeTuneControlRandom(maxit = 100L)


tune <- tuneParams(learner = rf_lrn, task = traintask, resampling = rdesc, measures = list(mse), par.set = ps, control = ctrl, show.info = T)

```

```{r}
# the mse is little better than previous
```


```{r}
set.seed(2)
formula <- "SalePrice ~.-SalePrice"
opt_rf <- randomForest(as.formula(formula),data = train_data, ntree = 116, 
                       mtry = 11, nodesize = 10, importance = T)
```

```{r}
plot(opt_rf)
```

```{r}
imp_ord <- order(-opt_rf$importance[, "%IncMSE"])
opt_rf$importance[imp_ord,]
```

```{r}
varImpPlot(x = opt_rf, sort = T, n.var = 10)
```


```{r}
# make estimate on test data
est_test_sp <- predict(object = opt_rf, newdata = test_data)
mean((est_test_sp - test_data[,"SalePrice"]) ^ 2)
```


```{r}
# make prediction
pre_sale_price <- predict(object = opt_rf, newdata = pre_x)
result <- data.frame(Id = c(1461:2919), SalePrice = exp(pre_sale_price))
write.csv(x = result, file = "H:/kaggle/houseprice/data/submission_6.csv",
          row.names = FALSE)
# Your submission scored 0.14541, not good.
```


