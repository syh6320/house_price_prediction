---
title: "homework3"
author: "syh"
date: "May 22, 2017"
output: pdf_document
---

```{r}
# in this version, we use combine common methods and mice to deal with nas
# to manipulate data set
```


```{r}
# get data
raw_train <- read.csv(file = "G:/kaggle/houseprice/data/train.csv",
                      stringsAsFactors = FALSE)

raw_test <- read.csv(file = "G:/kaggle/houseprice/data/test.csv", stringsAsFactors = F)

raw_test$SalePrice <- rep(0,dim(raw_test)[1])

all_data <- rbind(raw_train, raw_test)
```

```{r}
# deal with NA value
# first have a look which columns have NAs
na_sort <- sapply(all_data, function(x){
  sum(is.na(x))
})

na_sort
```
```{r}
# at first we remove columns with na in excess of 5% of all
keep_col <- which(na_sort < dim(all_data)[1] * 0.05)
all_data <- all_data[keep_col]
```


```{r}
# check other columns with NAs
sort(sapply(all_data, function(x){
  sum(is.na(x))
}), decreasing = TRUE)
```


```{r}
# we can find that there are lots of columns related with basement with nas.
# these missing value can be due to not being exist
all_data[is.na(all_data$BsmtCond),
              c("BsmtExposure","BsmtQual","BsmtFinType2", "BsmtFullBath", "BsmtFinType1","BsmtHalfBath","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF")] 
# there is no basement for these houses
```

```{r}
# we can create a another type value, "None" or 0 for these NAs.

all_data[is.na(all_data$BsmtCond), "BsmtCond"] <- "None"
all_data[is.na(all_data$BsmtExposure), "BsmtExposure"] <- "None"
all_data[is.na(all_data$BsmtQual), "BsmtQual"] <- "None"
all_data[is.na(all_data$BsmtFinType2), "BsmtFinType2"] <- "None"
all_data[is.na(all_data$BsmtFinType1), "BsmtFinType1"] <- "None"

all_data[is.na(all_data$BsmtHalfBath), "BsmtHalfBath"] <- 0
all_data[is.na(all_data$BsmtFinSF1), "BsmtFinSF1"] <- 0
all_data[is.na(all_data$BsmtFinSF2), "BsmtFinSF2"] <- 0
all_data[is.na(all_data$BsmtUnfSF), "BsmtUnfSF"] <- 0
all_data[is.na(all_data$TotalBsmtSF), "TotalBsmtSF"] <- 0
all_data[is.na(all_data$BsmtFullBath), "BsmtFullBath"] <- 0
```

```{r}
# let's deal with MasVnrType, MasVnrArea
all_data[is.na(all_data$MasVnrType),"MasVnrArea"]
# the same reason as basement
```

```{r}
table(all_data$MasVnrType)
all_data[is.na(all_data$MasVnrType), "MasVnrType"] <- "None"
all_data[is.na(all_data$MasVnrArea), "MasVnrArea"] <- 0
```

```{r}
# GarageCars, GarageSize,
# there are many features about garage, but only one of them is missing
# it's due to data transportation, probably

# it's same reason for Kitchen
all_data[is.na(all_data$KitchenQual),"KitchenAbvGr"]

# same with features about exterior
all_data[is.na(all_data$Exterior1st), c("ExterCond","Exterior2nd","ExterQual")]

# so we plan to use mice to impute these values from other features
```



```{r}
# first we should convert character to factor
cha_col <- c("MSSubClass", "MSZoning", "Street", "LotShape", "LandContour",
            "Utilities", "LotConfig", "LandSlope", "Neighborhood", "Condition1", "Condition2","BldgType", "HouseStyle", "OverallQual", "OverallCond", "RoofStyle", "RoofMatl","Exterior1st", "Exterior2nd", "MasVnrType", "ExterQual", "ExterCond", "Foundation","BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Heating","HeatingQC", "CentralAir", "Electrical", "KitchenQual", "Functional", 
"PavedDrive", "MoSold", "SaleType", "SaleCondition")
all_data[cha_col] <- lapply(all_data[cha_col], as.factor)
```

```{r}
library(mice)
```

```{r}
# impute nas by mice
im_all_data <- mice(data = all_data, m = 1, method = "cart")
real_all_data <- complete(im_all_data)
```


```{r}
# check again if there is no missing value
sort(sapply(real_all_data, function(x){sum(is.na(x))}), decreasing = TRUE)
# there isn't missing value any more.
```


```{r}

# record real_all_data data set
write.csv(file = "G:/kaggle/houseprice/data/real_all_data_hybrid.csv", x = real_all_data)

```

```{r}
#create real_all_data without id
real_all_data <- subset(real_all_data, select = -Id)
```


```{r}
# feature engineering
# how many years are these houses
# train_no_miss$Age <- 2017 - train_no_miss[,"YearBuilt"]

# total Floor square feet + basement
# train_no_miss$tot_Flo_area <- train_no_miss$X1stFlrSF 
                # + train_no_miss$X2ndFlrSF 
                # + train_no_miss$TotalBsmtSF

# how many years house last since repairing
# train_no_miss$rep_yea <- 2017 - train_no_miss$YearRemodAdd
```


```{r}
# transform sale price to more normal, 
# in order to subject to assumptin of linear regression
real_all_data$SalePrice <- log(real_all_data$SalePrice)
```

```{r}
# plot(density(whole_train$SalePrice))
```


```{r}
# train a simple linear regression model first
simple_lm <- lm(SalePrice ~ ., data = real_all_data[c(1:1460),])
                
summary(simple_lm)
```


```{r}
# residual plot
# 1st: residual is unbais and homoscedastic, 
# 2nd: basically, residual is subject to normal distribution which means variacne of error is constant.
# 4th: we know some outlier: 89, 826, 524 high leaverage and high residual
plot(simple_lm)
```


```{r}
# we can draw conclusion that basically underlying relations is linear
# Adjusted R-squared:  0.9334, it seems it's pretty good.
# F-statistic: 82.41, it's not very high, though.
```


```{r}
# scale numerical features
# for purpose of comparing predection power of different features
# num_col <- setdiff(colnames(whole_train),cha_col)
# whole_train[setdiff(num_col,"SalePrice")] <- sapply(whole_train[setdiff(num_col,"SalePrice")], scale)
```


```{r}

# get ind and dep data
dep_data <- real_all_data$SalePrice
# x must be a matrix for glmnet
ind_data <- model.matrix(~., subset(real_all_data, select = -SalePrice))


# split all data into for training model and for prediction
x_model <- ind_data[c(1:dim(raw_train)[1]),]
y_model <- dep_data[c(1:dim(raw_train)[1])]
  

x_pre <- ind_data[-c(1:dim(raw_train)[1]),]

# split training data into train and test
set.seed(1000)
train_ind <- sample(x = 1:dim(x_model)[1], size = dim(x_model)[1] * 0.7)

x_train <- x_model[train_ind,]
y_train <- y_model[train_ind]

x_test <- x_model[-train_ind,]
y_test <- y_model[-train_ind]


```


```{r}
# resolve multicolinearity and select lamda
# we choose different penality methods
library(glmnet)
lasso_lm <- glmnet(x = x_train, y = y_train, alpha = 1)
ridge_lm <- glmnet(x = x_train, y = y_train, alpha = 0)
elnet_lm <- glmnet(x = x_train, y = y_train, alpha = 0.5)
```


```{r}
# train 11 models with different alpha, different penalty, ranging from 0 to 1
# by cross validation,default folders are 10
for(i in c(0:10)){
  assign(paste("cvglm", i,sep = ""), 
         cv.glmnet(x = x_test, y = y_test, alpha = i/10,
                  type.measure = "mse", family = "gaussian"))
}
```


```{r}
par(mfrow=c(1,2))
plot(lasso_lm, xvar = "lambda", label = T)
plot(cvglm10)

```


```{r}
# let calculate the mse of these three models
lasso_y <- predict.glmnet(object = lasso_lm, newx = x_test, s = cvglm10$lambda.1se)
mean((y_test - lasso_y)^2)
```


```{r}
par(mfrow=c(1,2))
plot(ridge_lm, xvar = "lambda", label = T)
plot(cvglm0)
```


```{r}
ridge_y <- predict.glmnet(object = ridge_lm, newx = x_test, s = cvglm0$lambda.1se)
mean((y_test - ridge_y)^2)
```


```{r}
elnet_y <- predict.glmnet(object = elnet_lm, newx = x_test, s = cvglm5$lambda.1se)
mean((y_test - elnet_y)^2)
```

```{r}
# it seems that performance of elastic net is best
```


```{r}
# let choose the optimal lambda from elastic models
cvglm <- list(cvglm0,cvglm1,cvglm2,cvglm3,cvglm4,cvglm5,
           cvglm6,cvglm7,cvglm8,cvglm9,cvglm10
           )
mse <- NULL
for(i in c(1:11)){
  y_pre <- predict.cv.glmnet(object = cvglm[[i]],newx = x_test,
                             s = cvglm[[i]]$lambda.1se)
  mse <- c(mse, mean((y_test - y_pre)^2))
}
```

```{r}
plot(x = c(0:10), y = mse, xlab = "alpha", ylab = "mse")
```

```{r}
# print the model's index with optimal lambda
which.min(mse) - 1
min(mse)
```


```{r}
# use prediction on test data using our optimal model
pre_pri <- predict.cv.glmnet(object = cvglm5, newx = x_pre, 
                             s = cvglm5$lambda.1se)
```


```{r}
result <- data.frame(Id = raw_test$Id, SalePrice = exp(pre_pri))
names(result) <- c("Id", "SalePrice")
```

```{r}
# write result back to csv
write.csv(x = result, file = "H:/kaggle/houseprice/data/submission_1.csv",
          row.names = FALSE)
```

